# -*- coding: utf-8 -*-
"""mlassignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gPhyTPQ1SdkD6uwuZzc0028KNIAhyYpJ
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df= pd.read_csv(r'C:\Users\vishal\Downloads\archive (2)\train.csv')

df

#EDA

df.head()

df.tail()

df.info()

df.describe()

df.shape

df.dtypes

# df['price_range'] = df['price_range'].astype(int)

df.isnull().sum()

df["price_range"].value_counts()

mode_price_range = df['price_range'].mode()[0]
df['price_range'].fillna(mode_price_range, inplace=True)

#another option to delete rows

df["price_range"].value_counts()

df.isnull().sum()

corr_matrix = df.corr()
plt.figure(figsize=(20, 15))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Matrix Heatmap')
plt.show()

#MODEL

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

X = df.drop(columns=['price_range'])
y = df['price_range']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LogisticRegression()        #100
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)

print(f'Accuracy: {accuracy:.2f}')
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))



model1 = LogisticRegression(max_iter=100)                     #iteration
model1.fit(X_train, y_train)

accuracy = accuracy_score(y_test, y_pred)

print(f'Accuracy: {accuracy:.2f}')
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

#DICISION TREE try

from sklearn.tree import DecisionTreeClassifier

dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train, y_train)
y_pred = dt_classifier.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)

print(f'Accuracy: {accuracy:.2f}')
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

#random forest

from sklearn.ensemble import RandomForestClassifier

X = df.drop(columns=['price_range'])
y = df['price_range']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model_rf = RandomForestClassifier(n_estimators=100, random_state=42)
model_rf.fit(X_train, y_train)

y_pred_rf = model_rf.predict(X_test)
accuracy_rf = accuracy_score(y_test, y_pred_rf)

print(f'Random Forest Accuracy: {accuracy_rf:.2f}')
print(classification_report(y_test, y_pred_rf))
print(confusion_matrix(y_test, y_pred_rf))

#LOGITIC REGRESSION GIVE GOOD RESULTS SO SAVE THIS MODEL

import pickle

model_file_path = 'C:/Users/vishal/Downloads//data science/nlp&ankit/Untitled Folder/newdeployment/logistic_regression_model.pkl'

with open(model_file_path, 'wb') as file:
    pickle.dump(model, file)



df.columns

#logistic regression train on 20 feature if we have to deploy this model so that time we have to give 20 value to model for prediction. so here iam try to calculate most imp 7 feature for
#try to calculate most important feature that mostly corealte with y
#use random forest for calculate most imp 7 feature

features = df.drop(columns=['price_range'])
target = df['price_range']

model = RandomForestClassifier(random_state=42)               #sample42tree
model.fit(features, target)

feature_importances = model.feature_importances_
feature_importance_df = pd.DataFrame({'Feature': features.columns,'Importance': feature_importances})             #importance scores

feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)
top_7_features = feature_importance_df.head(7)['Feature'].tolist()

print("Top 7 most important features:")
for feature in top_7_features:
    print(f"- {feature}")

features = ['ram', 'battery_power', 'px_height', 'px_width', 'mobile_wt', 'int_memory','pc']
df_top = df[features]

#logistic regresion model on this new dataframe

X1 = df_top
y = df['price_range']

X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.2, random_state=42)

model = LogisticRegression(max_iter=100, random_state=42)                        #max_iter
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f'Accuracy: {accuracy:.2f}')
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

#this not giving good result so try scalling option

from sklearn.preprocessing import StandardScaler

df_selected_features = df[features]
scaler = StandardScaler()

scaled_features = scaler.fit_transform(df_selected_features)
df_scaled_features = pd.DataFrame(scaled_features, columns=features)

df_scaled_features

X1 = df_scaled_features
y = df['price_range']

X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.2, random_state=42)

model = LogisticRegression(max_iter=1000, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)

print(f'Accuracy: {accuracy:.2f}')
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

#trying another model

from sklearn.tree import DecisionTreeClassifier

X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.2, random_state=42)
model_new = DecisionTreeClassifier(random_state=42)

model_new.fit(X_train, y_train)
y_pred = model_new.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f'Accuracy: {accuracy:.2f}')
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

#for testing we import another data set

df11= pd.read_csv(r'C:\Users\vishal\Downloads\archive (2)\test.csv')
df11=df11.drop(columns=['id'])

columns = ["ram", "battery_power", "px_height", "px_width", "mobile_wt", "int_memory","pc"]

a_ = df11[columns].copy()
a= a_.sample(n=10, random_state=42)
a

predictions = model.predict(a)

print("Predictions:", predictions)

#chance to overfitting so apply l1 regulization

model12 = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000, random_state=42)

model12.fit(X_train, y_train)
# print(X_train)

y_pred = model12.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f'Accuracy: {accuracy:.2f}')
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

predictions = model12.predict(a)

print("Predictions:", predictions)

model_file_path = 'C:/Users/vishal/Downloads//data science/nlp&ankit/Untitled Folder/newdeployment/newlogistic_regression_model.pkl'

with open(model_file_path, 'wb') as file:
    pickle.dump(model12, file)



